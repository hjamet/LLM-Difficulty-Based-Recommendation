# Project Context
- Text simplification project for French texts
- Goal: Simplify texts while preserving meaning, reducing CEFR level by one step

## Current State
- Core pipeline implemented
- Metrics computation implemented
- LaTeX table generation working

## Technical Details
### Metrics from d_Metrics.ipynb
1. Accuracy Metric (BERT):
   - Uses BERT model from difficulty_estimation
   - Measures if simplified text is at correct level
   - Score between 0 and 1
   - Downloaded from HuggingFace

2. Similarity Metric (BERT only):
   - Uses BERT embeddings cosine similarity
   - Measures meaning preservation
   - Score between 0 and 1
   - Ignore BLEU/ROUGE alternatives

3. Combined Metric Formula:
   ```
   score = (2 * w1 * A * (1-w1) * S) / (w1 * A + (1-w1) * S)
   where:
   - w1 = 0.8 (fixed)
   - A: accuracy score from BERT
   - S: similarity score from BERT embeddings
   ```

### Implementation Status
1. Core Functions:
   - compute_metrics(): ✓ Implemented
     - Load BERT model ✓
     - Calculate accuracy ✓
     - Calculate similarity ✓
     - Combine with w1=0.8 ✓
   - load_old_experiments_metrics(): ✓ Implemented (hardcoded)
   - create_latex_table(): ✓ Implemented

2. Data Flow:
   - Input: Predictions from gpt_evaluation.py
   - Process: Calculate metrics using BERT
   - Output: Combined scores in LaTeX table with blue scale colors
