# Project Context
- Text simplification project for French texts
- Goal: Simplify texts while preserving meaning, reducing CEFR level by one step
- Previous work:
  1. Dataset Creation (a_DatasetCreation.ipynb):
     - Used GPT-4 to generate training data
     - 30 sentences per CEFR level (A2->A1, B1->A2, etc.)
     - Each sentence with different topic
     - Structured prompt for consistent generation

  2. Model Training:
     - Fine-tuned Mistral-7B (b_FineTuningMistral.ipynb)
     - Fine-tuned OpenAI models (e_OpenAIFineTuning.ipynb)
       - Davinci
       - GPT-3.5
     - Used LoRA for efficient training
     - System prompt with CEFR context

  3. Evaluation (c_MistralEvaluation.ipynb, f_OpenAIEvaluation.ipynb):
     - Zero-shot and fine-tuned performance
     - Multiple models compared
     - Metrics saved in results/text_simplification/

## Current State
- Need to extend evaluation to GPT-4o models
- Need to update visualization style
- Have working pipeline for:
  - Data generation
  - Model fine-tuning
  - Evaluation
  - Results visualization

## Technical Details
### Training Format
- System prompt explaining CEFR levels
- Input: Original text with CEFR level
- Output: Simplified text one level lower
- JSON format for different models

### Evaluation Pipeline
1. Zero-shot testing
2. Fine-tuning preparation
3. Model training
4. Fine-tuned evaluation
5. Results comparison

## Next Steps
1. GPT-4o Models:
   - Use existing pipeline
   - Follow same evaluation protocol
   - Compare with previous models

2. Table Updates:
   - Match difficulty estimation style
   - Add color gradients
   - Implement best result highlighting
