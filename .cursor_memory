# Project Context
- Text simplification project for French texts
- Goal: Simplify texts while preserving meaning, reducing CEFR level by one step

## Current State
- Core pipeline implemented
- Ready to implement metrics computation

## Technical Details
### Metrics from d_Metrics.ipynb
1. Accuracy Metric (BERT):
   - Uses BERT model from difficulty_estimation
   - Measures if simplified text is at correct level
   - Score between 0 and 1
   - Downloaded from HuggingFace

2. Similarity Metric (BERT only):
   - Uses BERT embeddings cosine similarity
   - Measures meaning preservation
   - Score between 0 and 1
   - Ignore BLEU/ROUGE alternatives

3. Combined Metric Formula:
   ```
   score = (2 * w1 * A * (1-w1) * S) / (w1 * A + (1-w1) * S)
   where:
   - w1 = 0.5 (fixed)
   - A: accuracy score from BERT
   - S: similarity score from BERT embeddings
   ```

### Implementation Plan
1. Core Functions:
   - compute_metrics(): Main function
     - Load BERT model
     - Calculate accuracy
     - Calculate similarity
     - Combine with w1=0.5
   - load_old_experiments_metrics()
   - create_latex_table()

2. Data Flow:
   - Input: Predictions from gpt_evaluation.py
   - Process: Calculate metrics using BERT
   - Output: Combined scores in standard format
