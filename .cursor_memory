# Project Context
- Working on GPT model fine-tuning for French text difficulty estimation
- Using CEFR levels (A1-C2) and level1-level4 classifications
- Need to compute and compare metrics across different models and datasets

## Current State
- Completed model predictions generation with gpt_evaluation.py
- Need to implement metrics computation and comparison
- Three main functions to implement:
  1. compute_metrics(): Process new model predictions
  2. load_old_experiments_metrics(): Load historical results
  3. create_latex_table(): Generate formatted LaTeX table

## Technical Details
### Data Structure
- Prediction files location: results/dmkd_additions/difficulty_estimation/gpt_evaluation/
- File naming pattern: {dataset}_{model_id}{system_prompt_suffix}_predictions.csv
- Each file contains:
  - Original text
  - True difficulty level
  - Predicted difficulty level

### Metrics Requirements
- Primary metric: Accuracy
- Need to handle different difficulty scales:
  - CEFR (A1-C2) for french_difficulty and sentences
  - Level1-4 for ljl dataset
- Results must be comparable across all models

### LaTeX Output Requirements
- Need to generate properly formatted LaTeX table
- Should combine both new and historical results
- Must include:
  - Model names
  - Context usage (with/without)
  - Accuracy scores per dataset

## Performance Tracking
- Will need to track:
  - Accuracy scores across all models
  - Impact of context on performance
  - Comparison with baseline models
  - Statistical significance of improvements

## Next Steps
1. Implement file loading and processing in compute_metrics()
2. Create data structure for historical results
3. Design LaTeX table format
4. Add comprehensive logging
