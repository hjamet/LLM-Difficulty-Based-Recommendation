# Project Context
- Working on GPT model fine-tuning for French text difficulty estimation
- Using CEFR levels (A1-C2) and level1-level4 classifications
- Need to compute and compare metrics across different models and datasets

## Current State
- Implementation completed and working:
  1. compute_metrics(): 
     - Processes prediction files
     - Handles label normalization
     - Computes accuracy and pairwise mismatch
     - Saves results in compute_metrics/compute_metrics/
  2. load_old_experiments_metrics():
     - Loads historical results from multiple sources
     - Normalizes dataset and model names
     - Successfully loads all pairwise mismatch scores
     - Saves results in compute_metrics/load_old_experiments_metrics/
  3. create_latex_table():
     - Combines current and historical results
     - Generates formatted LaTeX tables
     - Saves tables in compute_metrics/create_latex_table/
- All metrics working correctly and saved properly

## Technical Details
### Data Structure
- Results directory structure:
  - compute_metrics/compute_metrics/: Current metrics
  - compute_metrics/load_old_experiments_metrics/: Historical metrics
  - compute_metrics/create_latex_table/: LaTeX tables and combined metrics
- File formats:
  - Metrics: CSV files with dataset, model, context, accuracy, pairwise_mismatch
  - Tables: LaTeX files with formatted results

### Model Performance
- Best performing models by dataset:
  - SentencesInternet: GPT-4o (acc: 0.9021, pw: 17.8917)
  - LjL: GPT-4o (acc: 0.7651, pw: 6.2131)
  - SentencesBooks: GPT-4o-mini (acc: 0.6156, pw: 34.3521)

## Next Steps
1. Add comprehensive testing
2. Enhance documentation
3. Analyze results for paper
4. Create visualizations
