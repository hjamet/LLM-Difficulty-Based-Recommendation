# Project Structure
dmkd_additions/
└── difficulty_estimation/
    ├── compute_metrics.py         # Metrics computation
    │   ├── _normalize_model_name()    # Model name standardization
    │   ├── _normalize_dataset_name()  # Dataset name standardization
    │   ├── _normalize_context()       # Context standardization
    │   ├── _parse_filename()          # Filename metadata extraction
    │   ├── pairwise_mismatch()        # Pairwise mismatch calculation
    │   ├── compute_metrics()          # Process new predictions
    │   └── load_old_experiments_metrics() # Load historical data
    ├── gpt_evaluation.py         # Prediction generation
    └── results/                  # Results storage
        ├── gpt_evaluation/       # New prediction files
        ├── OpenAiEvaluation/     # OpenAI historical results
        ├── OpenSourceModelsEvaluation/ # CamemBERT & Mistral results
        └── PairwiseMismatch/     # Pairwise mismatch results
            ├── openai_pairwise_mismatch.csv
            ├── bert_pairwise_mismatch.csv
            ├── mistral_pairwise_mismatch.csv
            └── readability_index_pairwise_mismatch.csv

# Key Components
## Metrics Computation (compute_metrics.py)
### Core Functions
1. compute_metrics()
   - Scans for prediction files
   - Normalizes labels for comparison
   - Calculates accuracy scores
   - Computes pairwise mismatch
   - Returns formatted DataFrame

2. load_old_experiments_metrics()
   - Loads multiple historical sources
   - Normalizes model and dataset names
   - Successfully loads pairwise mismatch scores
   - Handles all model types correctly

3. pairwise_mismatch()
   - Implements pairwise mismatch metric
   - Handles label encoding
   - Computes ordering errors
   - Returns normalized score

### Utility Functions
1. _normalize_model_name()
   - Handles fine-tuned models (GPT-4o, GPT-4o-mini)
   - Standardizes OpenAI model names
   - Maintains consistent naming convention

2. _normalize_dataset_name()
   - Unifies french difficulty variations
   - Maintains consistent dataset naming
   - Handles string conversion

3. _normalize_context()
   - Standardizes context names
   - Handles CECRL/empty variations
   - Ensures consistent format

4. _parse_filename()
   - Extracts dataset, model, and context
   - Handles different filename formats
   - Supports system prompt detection

## Data Flow
1. Predictions generated by gpt_evaluation.py
2. Results saved as CSV files
3. compute_metrics.py processes files
   - Normalizes names and labels
   - Computes accuracy metrics
   - Calculates pairwise mismatch
4. Historical results loaded and combined
5. Final results sorted and displayed

## Current Status
- All metrics working correctly
- All data loading issues resolved
- Results properly normalized
- Ready for paper generation

## Planned Improvements
- Clean up logging system
- Add comprehensive testing
- Enhance documentation
- Prepare final paper results
