# Project Structure
dmkd_additions/
└── difficulty_estimation/
    ├── compute_metrics.py         # Metrics computation
    │   ├── _normalize_model_name()    # Model name standardization
    │   ├── _normalize_dataset_name()  # Dataset name standardization
    │   ├── _normalize_context()       # Context standardization
    │   ├── _parse_filename()          # Filename metadata extraction
    │   ├── pairwise_mismatch()        # Pairwise mismatch calculation
    │   ├── compute_metrics()          # Process new predictions
    │   ├── load_old_experiments_metrics() # Load historical data
    │   └── create_latex_table()       # Generate LaTeX tables
    └── results/                  # Results storage
        └── compute_metrics/      # Metrics computation results
            ├── compute_metrics/          # Current metrics
            │   └── current_metrics.csv
            ├── load_old_experiments_metrics/ # Historical metrics
            │   └── historical_metrics.csv
            └── create_latex_table/       # LaTeX tables
                ├── tables/
                │   ├── sentencesbooks_metrics.tex
                │   ├── ljl_metrics.tex
                │   └── sentencesinternet_metrics.tex
                └── combined_metrics.csv

# Key Components
## Metrics Computation (compute_metrics.py)
### Core Functions
1. compute_metrics()
   - Scans for prediction files
   - Normalizes labels for comparison
   - Calculates accuracy scores
   - Computes pairwise mismatch
   - Saves results in dedicated directory

2. load_old_experiments_metrics()
   - Loads multiple historical sources
   - Normalizes model and dataset names
   - Successfully loads pairwise mismatch scores
   - Saves results in dedicated directory

3. create_latex_table()
   - Combines current and historical results
   - Generates formatted LaTeX tables
   - Handles duplicates
   - Saves tables and combined metrics

### Utility Functions
1. _normalize_model_name()
   - Handles fine-tuned models (GPT-4o, GPT-4o-mini)
   - Standardizes OpenAI model names
   - Maintains consistent naming convention

2. _normalize_dataset_name()
   - Unifies french difficulty variations
   - Maintains consistent dataset naming
   - Handles string conversion

3. _normalize_context()
   - Standardizes context names
   - Handles CECRL/empty variations
   - Ensures consistent format

4. _parse_filename()
   - Extracts dataset, model, and context
   - Handles different filename formats
   - Supports system prompt detection

## Data Flow
1. Predictions generated by gpt_evaluation.py
2. Results saved as CSV files
3. compute_metrics.py processes files
   - Normalizes names and labels
   - Computes metrics
   - Saves results in proper directories
4. Historical results loaded and combined
5. LaTeX tables generated and saved

## Current Status
- All functions working correctly
- Results properly saved and organized
- Ready for paper preparation

## Planned Improvements
- Add comprehensive testing
- Enhance documentation
- Create visualizations
- Analyze results for paper
