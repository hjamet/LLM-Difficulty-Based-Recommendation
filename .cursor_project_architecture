# Project Structure
dmkd_additions/text_simplification/
├── download_data.py               # Data download (✓ implemented)
├── gpt_fine_tuning.py            # Model training (✓ implemented)
├── gpt_evaluation.py             # Model evaluation (to implement)
│   ├── generate_predictions()    # Main evaluation function
│   │   ├── Input: model_id, dataset, system_prompt
│   │   └── Output: predictions DataFrame
│   └── __main__                  # Evaluation script
│       ├── Test both models
│       └── Save results properly
│
└── results/                      # Results storage
    ├── gpt_fine_tuning/         # Fine-tuning results (✓ done)
    │   ├── backups/             # Configuration backups
    │   └── *_job.json           # Job details
    └── gpt_evaluation/          # Evaluation results
        ├── predictions/         # Model predictions
        │   ├── zero_shot/      # Zero-shot results
        │   └── fine_tuned/     # Fine-tuned results
        └── metrics/            # Evaluation metrics

# Components
## Evaluation Pipeline
1. Data Preparation
   - Load test datasets
   - Track CEFR levels
   - Format for simplification

2. Model Evaluation
   - System prompt + simple user prompt
   - Generate predictions
   - Save results by model/dataset

3. Results Processing
   - Consistent file naming
   - Include metadata
   - Enable analysis

## Integration Points
1. With download_data.py:
   - Access to test datasets
   - Consistent data format

2. With gpt_fine_tuning.py:
   - Access to model IDs
   - Reuse system prompt
   - Consistent formats
