# Project Structure
dmkd_additions/
└── difficulty_estimation/
    ├── compute_metrics.py         # Metrics computation
    │   ├── compute_metrics()      # Process new predictions
    │   ├── load_old_experiments_metrics() # Load historical data
    │   └── create_latex_table()   # Generate LaTeX output
    ├── gpt_evaluation.py         # Prediction generation
    └── results/                  # Results storage
        └── gpt_evaluation/       # Prediction files
            ├── {dataset}_{model}_predictions.csv
            └── historical_results/

# Key Components
## Metrics Computation (compute_metrics.py)
- Purpose: Calculate and compare model performance
- Input: Prediction files from gpt_evaluation.py
- Output: DataFrame with accuracy metrics
- Dependencies: pandas, numpy

### Functions
1. compute_metrics()
   - Loads prediction files
   - Calculates accuracy scores
   - Handles different difficulty scales
   - Returns formatted DataFrame

2. load_old_experiments_metrics()
   - Loads historical results
   - Formats data consistently
   - Handles legacy data structures

3. create_latex_table()
   - Combines new and historical results
   - Generates formatted LaTeX code
   - Ensures consistent styling

## Data Flow
1. Predictions generated by gpt_evaluation.py
2. Results saved as CSV files
3. compute_metrics.py processes files
4. Results combined with historical data
5. LaTeX table generated for paper

## Planned Improvements
- Add comprehensive logging
- Implement error handling
- Add validation checks
- Support for additional metrics
